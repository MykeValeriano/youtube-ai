import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# import os
# from langchain_community.vectorstores import FAISS
# from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
# from dotenv import load_dotenv
# import sys
# # sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))


# # Load environment variables from .env file (like your GOOGLE_API_KEY)
# load_dotenv()

# def query_video(question: str, vectorstore: FAISS, top_k: int = 3) -> str:
#     """
#     Given a user question and a FAISS vectorstore of embedded transcript chunks,
#     perform a semantic search to find relevant transcript sections and
#     generate an answer using the Google Gemini chat model.

#     Args:
#         question (str): The natural language question to answer.
#         vectorstore (FAISS): The FAISS vectorstore containing embedded transcript chunks.
#         top_k (int): Number of most relevant chunks to retrieve for context (default: 3).

#     Returns:
#         str: The answer generated by the Gemini LLM based on retrieved context.
#     """

#     # Step 1: Initialize Gemini embeddings with your API key
#     embeddings = GoogleGenerativeAIEmbeddings(
#         model="models/embedding-001",
#         google_api_key=os.getenv("GOOGLE_API_KEY")
#     )

#     # Step 2: Embed the question into a vector representation
#     question_vector = embeddings.embed_query(question)

#     # Step 3: Perform similarity search on the FAISS vectorstore
#     # Retrieve top_k most relevant document chunks based on the question embedding
#     docs = vectorstore.similarity_search_by_vector(question_vector, k=top_k)

#     # Step 4: Concatenate the text content of retrieved chunks to build context for LLM
#     context = "\n\n".join([doc.page_content for doc in docs])

#     # Step 5: Initialize the Gemini chat LLM for generating the answer
#     llm = ChatGoogleGenerativeAI(
#     model="gemini-1.5-flash",  # or "gemini-1.5-pro" if supported
#     google_api_key=os.getenv("GOOGLE_API_KEY"),
#     temperature=0.2  # Lower temperature for factual answers
#     )

#     # Step 6: Create a prompt combining the context and the user question
#     custom_prompt = PromptTemplate(
#     input_variables=["context", "question"],
#     template="""
#         You are a helpful AI tutor. Based only on the CONTEXT below, answer the QUESTION.

#         CONTEXT:
#         {context}

#         QUESTION:
#         {question}

#         Answer:
#         """
#         )

#     # Step 7: Invoke the Gemini LLM with the prompt and get the generated answer
#     response = llm.invoke(custom_prompt)

#     # Return only the generated text content as the final answer
#     return response.text

# # Example usage:
# if __name__ == "__main__":
#     from rag.rag_agent import load_vector_db

#     # Example transcript (replace with actual transcript or longer text)
#     sample_text = """Python is a versatile programming language that supports object-oriented, procedural, and functional programming.
#     It is widely used in machine learning, automation, and backend development."""

#     # Load the vectorstore from the sample transcript
#     vectorstore = load_vector_db(sample_text)

#     # Sample question to test the query function
#     question = "What is Python used for?"

#     # Get the answer from the RAG query function
#     answer = query_video(question, vectorstore)

#     # Print question and answer to console
#     print("Q:", question)
#     print("A:", answer)


# agents/query_agent.py

import os
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from rag.rag_agent import load_vector_db
from utils.youtube_agent import get_transcript_from_youtube

load_dotenv()  # Load API keys from .env file

# ----------------------------------------
# Configure LLM (Gemini Pro or Gemini Flash)
# ----------------------------------------
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",  # or "gemini-1.5-pro" if supported
    google_api_key="AIzaSyBcRtLFcwuHt2cMN_1Qf9G0xn48w1usn4Y",

    temperature=0.2  # Lower temperature for factual answers
)

# ----------------------------------------
# Prompt Template for RAG
# ----------------------------------------
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
    You are a helpful AI tutor. Based only on the CONTEXT below, answer the QUESTION.

    CONTEXT:
    {context}

    QUESTION:
    {question}

    Answer:
    """
    )

# ----------------------------------------
# Query Function
# ----------------------------------------
def query_video(question: str, vectorstore: FAISS):
    """
    Takes a user question and vectorstore, performs similarity search + Gemini answer.
    """
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        chain_type="stuff",
        chain_type_kwargs={"prompt": custom_prompt}
    )
    return qa_chain.run(question)

# ----------------------------------------
# Test Run
# ----------------------------------------
if __name__ == "__main__":
    print("üîç Loading vector database...")
    
    # Load sample transcript ‚Äî you can replace with a real transcript string or fetch via YouTube
    sample_text = "Natural Language Processing is a subfield of AI focused on understanding human language."

    # youtube link
    source  =  "https://www.youtube.com/watch?v=igVbY8iXVmM"

    # importing the transcripts
    transcript = get_transcript_from_youtube(source)
        
    # Step 1: Embed and load into FAISS
    vectorstore = load_vector_db(transcript)
    print("‚úÖ Vector database loaded.")

    # Step 2: Ask a question
    question = input("‚ùì Ask your question about the video: ")
    answer = query_video(question, vectorstore)

    print("\nüß† Answer:")
    print(answer)



# from agents.summary_agent import generate_summary
# from agents.notes_agent import generate_notes

# summary = generate_summary(source)
# notes = generate_notes(source)

# print("summary:\n", summary)
# print("notes:\n", notes)